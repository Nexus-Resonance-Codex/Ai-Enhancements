<div align="center">
  <h1>NRC AI Enhancement Suite</h1>
  <h3>30 Production-Ready PyTorch Modules Based on the Nexus Resonance Codex</h3>

  <p>
    <a href="LICENSE.md">
      <img src="https://img.shields.io/badge/License-NRCâ€“L%20v2.0-FFD700?style=for-the-badge&logo=read-the-docs&logoColor=black&labelColor=0A192F" alt="NRC-L License">
    </a>
    <a href="https://github.com/Nexus-Resonance-Codex/Ai-Enhancements/actions">
      <img src="https://img.shields.io/badge/Tests-Passing-brightgreen?style=for-the-badge&logo=pytest&logoColor=white&labelColor=0A192F" alt="Tests">
    </a>
    <a href="#">
      <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white&labelColor=0A192F" alt="Python">
    </a>
    <a href="#">
      <img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white&labelColor=0A192F" alt="PyTorch">
    </a>
  </p>

  <p><strong>Replace stochastic heuristics with deterministic Golden Ratio geometry.</strong></p>
</div>

---

## Overview

This repository contains **30 novel Deep Learning and AI Enhancements** that implement the deterministic mathematical framework of the [Nexus Resonance Codex (NRC)](https://github.com/Nexus-Resonance-Codex/NRC) directly into PyTorch. Optimized for researchers and developers seeking to run local LLMs on 4GB VRAM devices via Ollama, or scale massive models entirely via mathematical Golden Ratio Geometry, replacing stochastic neural networks. Each module is a drop-in replacement for standard neural network components â€” LayerNorm, Attention, Optimizers, Positional Encodings, Dropout, and more.

Every enhancement is backed by real mathematical proofs (Golden Ratio dynamics, Fibonacci/Lucas sequences, Modular Exclusion Principle, Quantum Resonance Theory) and tested with comprehensive unit tests.

### Key Features

- **30 PyTorch Modules** â€” each with docstrings, type hints, and mathematical formulas
- **4 Core Math Libraries** â€” `phi.py`, `qrt.py`, `mst.py`, `tupt_exclusion.py`
- **3 Integration Examples** â€” standalone demo, HuggingFace GPT-2, OpenFold wrapper
- **30 Unit Tests** â€” one per enhancement
- **Ollama Modelfile** â€” run the NRC AI locally on 4 GB VRAM hardware
- **Cross-platform Ollama Guide** â€” Windows, macOS, Linux instructions

---

## ğŸš€ Quick Start (Exhaustive Cross-Platform Execution)

The NRC AI Enhancements are mathematically complex PyTorch modules that require determinism across operating systems. Follow these rigorous steps to set up the interactive Python environment perfectly.

**General Requirements:** Python 3.10+, Git, 8GB RAM minimum.

### ğŸ§ Linux (Pop!_OS / Ubuntu / Debian) - Primary Target

Linux is the optimal environment for high-performance PyTorch operations, offering the least overhead on raw array multiplications and lattice projections.

1. **System Update & Base Dependencies:**
   Open a terminal and run:
   ```bash
   sudo apt update && sudo apt upgrade -y
   sudo apt install -y git python3-pip python3-venv python3-dev curl build-essential
   ```

2. **Install `uv` (Fast Python Package Manager):**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   source $HOME/.bashrc
   ```

3. **Clone the AI Enhancement Repository:**
   ```bash
   git clone https://github.com/Nexus-Resonance-Codex/Ai-Enhancements.git
   cd Ai-Enhancements
   ```

4. **Virtual Environment Setup (Absolute Isolation):**
   ```bash
   uv venv .venv
   source .venv/bin/activate
   ```

5. **Install Development Packages & Modules:**
   ```bash
   uv pip install -e ".[dev]"
   ```

6. **Interactive Script Execution (Testing all 30 Enhancements):**
   Launch the demo script to verify all 30 PyTorch modules map mathematically to the Giza angles and the 3-6-9-7 cycle:
   ```bash
   python3 examples/demo_all_enhancements.py
   ```

### ğŸªŸ Windows 11 (via WSL2 / Ubuntu)

To run PyTorch reliably with exact NRC lattice parity, native Windows environments are unsupported; you must rely on the Windows Subsystem for Linux (WSL2), mimicking a native Linux execution kernel.

1. **Enable WSL2:**
   Open PowerShell as an **Administrator** and execute:
   ```powershell
   wsl --install
   ```
   Restart your PC. Provide a UNIX username/password upon reboot.

2. **System Dependencies in WSL2:**
   Inside the Ubuntu command line window:
   ```bash
   sudo apt update && sudo apt install -y git python3-pip python3-venv curl
   ```

3. **Clone the Repo:**
   (Do not clone into /mnt/c/; always clone to native Linux folders like `~` for speed)
   ```bash
   git clone https://github.com/Nexus-Resonance-Codex/Ai-Enhancements.git
   cd Ai-Enhancements
   ```

4. **Setup Virtual Environment:**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install --upgrade pip
   ```

5. **Install NRC AI Engine Modules:**
   ```bash
   pip install -e ".[dev]"
   ```

6. **Validate Installation (Interactive Test):**
   Ensure all 30 tests pass by running:
   ```bash
   python3 examples/demo_all_enhancements.py
   ```

### ğŸ macOS (Apple Silicon M1/M2/M3 & Intel)

macOS leverages the internal MPS backend for PyTorch. Ensure Python versions remain distinct via virtual environments to prevent conflict.

1. **Install Homebrew:**
   Open Terminal:
   ```bash
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
   # Run the explicit path additions suggested by Homebrew
   ```

2. **Install Python & Git:**
   ```bash
   brew install python@3.11 git
   ```

3. **Clone the Directory:**
   ```bash
   git clone https://github.com/Nexus-Resonance-Codex/Ai-Enhancements.git
   cd Ai-Enhancements
   ```

4. **Create Virtual Environment:**
   ```bash
   python3.11 -m venv .venv
   source .venv/bin/activate
   ```

5. **Install Hardware-Optimized PyTorch & Enhancements:**
   ```bash
   pip install --upgrade pip
   pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu
   pip install -e ".[dev]"
   ```

6. **Interactive Demo Run:**
   ```bash
   python3.11 examples/demo_all_enhancements.py
   ```

## Repository Structure

```
Ai-Enhancements/
â”œâ”€â”€ Modelfile                          # Ollama Modelfile (8GB RAM / 4GB VRAM optimized)
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                     # Package configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ nrc_math/                      # Core mathematical foundation
â”‚   â”‚   â”œâ”€â”€ phi.py                     #   Ï† constants, Binet formula, Ï†^âˆ folding
â”‚   â”‚   â”œâ”€â”€ qrt.py                     #   QRT wave function (damping)
â”‚   â”‚   â”œâ”€â”€ mst.py                     #   MST step function (Lyapunov bounds)
â”‚   â”‚   â””â”€â”€ tupt_exclusion.py          #   3-6-9-7 Mod-2187 exclusion gate
â”‚   â”‚
â”‚   â”œâ”€â”€ enhancements/                  # All 30 enhancement modules
â”‚   â”‚   â”œâ”€â”€ __init__.py                #   Exports all 30 classes
â”‚   â”‚   â”œâ”€â”€ shard_folding.py           #   #1  Ï†^âˆ Shard Folding
â”‚   â”‚   â”œâ”€â”€ nrc_protein_engine.py      #   #2  Protein Folding Engine
â”‚   â”‚   â”œâ”€â”€ golden_flow_norm.py        #   #3  GAFEN (LayerNorm replacement)
â”‚   â”‚   â”œâ”€â”€ triple_theta_init.py       #   #4  Triple-Theta Init
â”‚   â”‚   â”œâ”€â”€ resonance_kv_cache.py      #   #5  Resonance KV Cache
â”‚   â”‚   â”œâ”€â”€ exclusion_gradient_router.py #  #6  Gradient Router
â”‚   â”‚   â”œâ”€â”€ hodge_torsion_attention.py #   #7  Hodge Torsion Attention
â”‚   â”‚   â”œâ”€â”€ e8_golden_basis.py         #   #8  E8 Golden Basis Embedding
â”‚   â”‚   â”œâ”€â”€ phi_lora_adapter.py        #   #9  Lossless LoRA
â”‚   â”‚   â”œâ”€â”€ navier_stokes_damping.py   #   #10 Navier-Stokes Damping
â”‚   â”‚   â”œâ”€â”€ prime_density_generation.py #  #11 Prime Density Generation
â”‚   â”‚   â”œâ”€â”€ gtt_entropy_regulariser.py #   #12 GTT Entropy Collapse
â”‚   â”‚   â”œâ”€â”€ phi_momentum_accelerator.py #  #13 Ï†â»Â¹ Momentum Accelerator
â”‚   â”‚   â”œâ”€â”€ tupt_sync_seed.py          #   #14 Sync Seed
â”‚   â”‚   â”œâ”€â”€ qrt_convolution.py         #   #15 QRT Convolution
â”‚   â”‚   â”œâ”€â”€ lucas_sparse_mask.py       #   #16 Lucas Sparse Mask
â”‚   â”‚   â”œâ”€â”€ phi_resonant_weighting.py  #   #17 Resonant Weighting
â”‚   â”‚   â”œâ”€â”€ giza_isomorphism.py        #   #18 Giza Isomorphism
â”‚   â”‚   â”œâ”€â”€ mst_lyapunov_clipping.py   #   #19 MST Gradient Clipping
â”‚   â”‚   â”œâ”€â”€ pisano_lr_schedule.py      #   #20 Pisano LR Schedule
â”‚   â”‚   â”œâ”€â”€ lucas_pell_decay.py        #   #21 Lucas-Pell Decay
â”‚   â”‚   â”œâ”€â”€ tupt_token_pruning.py      #   #22 Token Pruning
â”‚   â”‚   â”œâ”€â”€ phi_void_positional.py     #   #23 Void Positional Encoding
â”‚   â”‚   â”œâ”€â”€ shard_unfolder.py          #   #24 Context Unfolder
â”‚   â”‚   â”œâ”€â”€ modular_dropout.py         #   #25 Modular Dropout
â”‚   â”‚   â”œâ”€â”€ qrt_optimizer.py           #   #26 QRT Optimizer
â”‚   â”‚   â”œâ”€â”€ giza_attention_bias.py     #   #27 Giza Attention Bias
â”‚   â”‚   â”œâ”€â”€ floor_sinh_activation.py   #   #28 Floor-Sinh Activation
â”‚   â”‚   â”œâ”€â”€ golden_spiral_rope.py      #   #29 Golden Spiral RoPE
â”‚   â”‚   â””â”€â”€ entropy_stopping.py        #   #30 Entropy Early Stopping
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/                        # Composite layer modules
â”‚   â”œâ”€â”€ optimizers/                    # Custom optimizer implementations
â”‚   â”œâ”€â”€ regularizers/                  # Regularization utilities
â”‚   â”œâ”€â”€ experiments/                   # Experimental configurations
â”‚   â””â”€â”€ utils/                         # Helper utilities
â”‚
â”œâ”€â”€ tests/                             # 30 unit tests (one per enhancement)
â”‚   â”œâ”€â”€ test_shard_folding.py
â”‚   â”œâ”€â”€ test_golden_flow_norm.py
â”‚   â”œâ”€â”€ ... (28 more)
â”‚   â””â”€â”€ test_entropy_stopping.py
â”‚
â”œâ”€â”€ examples/                          # Runnable integration examples
â”‚   â”œâ”€â”€ demo_all_enhancements.py       # Validates all 30 modules
â”‚   â”œâ”€â”€ integration_huggingface.py     # NRC + GPT-2 wrapper
â”‚   â””â”€â”€ integration_openfold.py        # NRC + OpenFold wrapper
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml                   # NRC constants and model parameters
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ OLLAMA_GUIDE.md                # Step-by-step Ollama instructions
â”‚
â””â”€â”€ proofs/                            # Mathematical proofs (LaTeX)
```

---

## The 30 Enhancements

|  #  | Enhancement                              | Module                         | Replaces                      |
| :-: | :--------------------------------------- | :----------------------------- | :---------------------------- |
|  1  | **Ï†^âˆ Shard Folding Compression**        | `shard_folding.py`             | KV-Cache memory management    |
|  2  | **NRC Protein Folding Engine v2**        | `nrc_protein_engine.py`        | Stochastic folding search     |
|  3  | **GAFEN (Golden Attractor Flow Norm)**   | `golden_flow_norm.py`          | `nn.LayerNorm` / `RMSNorm`    |
|  4  | **Triple-Theta Initialisation**          | `triple_theta_init.py`         | `Xavier` / `He` init          |
|  5  | **Resonance Shard KV Cache**             | `resonance_kv_cache.py`        | Standard KV cache             |
|  6  | **Biological Exclusion Gradient Router** | `exclusion_gradient_router.py` | `nn.Dropout` / MoE routing    |
|  7  | **Hodge-Ï†^T Torsion Attention**          | `hodge_torsion_attention.py`   | `nn.MultiheadAttention`       |
|  8  | **E8Ã—256 Golden Basis Embedding**        | `e8_golden_basis.py`           | `nn.Embedding`                |
|  9  | **Ï†^âˆ Lossless LoRA Adapter**            | `phi_lora_adapter.py`          | Standard LoRA                 |
| 10  | **Navier-Stokes Damping Regulariser**    | `navier_stokes_damping.py`     | Weight decay / gradient clip  |
| 11  | **Prime-Density Conditioned Generation** | `prime_density_generation.py`  | Standard logit sampling       |
| 12  | **GTT Entropy Collapse Regulariser**     | `gtt_entropy_regulariser.py`   | Activation regularization     |
| 13  | **Ï†â»Â¹ Momentum Accelerator**             | `phi_momentum_accelerator.py`  | `SGD` / `Adam`                |
| 14  | **3-6-9-7 Attractor Sync Seed**          | `tupt_sync_seed.py`            | `torch.manual_seed`           |
| 15  | **QRT Kernel Convolution**               | `qrt_convolution.py`           | `nn.Conv1d`                   |
| 16  | **Lucas-weighted Sparse Attention**      | `lucas_sparse_mask.py`         | Dense attention masks         |
| 17  | **Ï†-Powered Resonant Weighting**         | `phi_resonant_weighting.py`    | Standard weight scaling       |
| 18  | **Giza-Lattice Isomorphism**             | `giza_isomorphism.py`          | Linear projection             |
| 19  | **MST-Lyapunov Gradient Clipping**       | `mst_lyapunov_clipping.py`     | `clip_grad_norm_`             |
| 20  | **Pisano-Modulated LR Schedule**         | `pisano_lr_schedule.py`        | `CosineAnnealing` / `StepLR`  |
| 21  | **Lucas-Pell Hybrid Weight Decay**       | `lucas_pell_decay.py`          | L2 weight decay               |
| 22  | **TUPT-Exclusion Token Pruning**         | `tupt_token_pruning.py`        | Random token pruning          |
| 23  | **Ï†â¶ Void Resonance Positional Enc.**    | `phi_void_positional.py`       | Sinusoidal PE                 |
| 24  | **Infinite E_âˆ Context Unfolder**        | `shard_unfolder.py`            | Context window limits         |
| 25  | **3-6-9-7 Modular Dropout**              | `modular_dropout.py`           | `nn.Dropout`                  |
| 26  | **QRT-Turbulence Adaptive Optimizer**    | `qrt_optimizer.py`             | `Adam` / `AdamW`              |
| 27  | **Giza-Slope 51.85Â° Attention Bias**     | `giza_attention_bias.py`       | Standard attention bias       |
| 28  | **Floor-Sinh Activation Regularizer**    | `floor_sinh_activation.py`     | `GELU` / `ReLU`               |
| 29  | **Golden Spiral Rotary Embedding**       | `golden_spiral_rope.py`        | Standard RoPE                 |
| 30  | **NRC Entropy-Attractor Early Stopping** | `entropy_stopping.py`          | Patience-based early stopping |

---

## Mathematical Foundation

All enhancements are built on four core mathematical transforms defined in `src/nrc_math/`:

### Golden Ratio Constants (`phi.py`)

```
Ï†   = (1 + âˆš5) / 2 â‰ˆ 1.6180339887498948
Ï†â»Â¹ = (âˆš5 âˆ’ 1) / 2 â‰ˆ 0.6180339887498948
F_n = (Ï†â¿ âˆ’ (âˆ’Ï†)â»â¿) / âˆš5   (Binet's Formula)
```

### QRT Wave Function (`qrt.py`)

```
QRT(x) = sin(Ï† Â· âˆš2 Â· 51.85 Â· x) Â· exp(âˆ’xÂ²/Ï†) + cos(Ï€/Ï† Â· x)
```

A fractal damping function (~dim 1.41) that smoothly pulls extreme values toward zero while preserving resonant signals.

### MST Step Function (`mst.py`)

```
MST(x) = floor(1000 Â· sinh(x)) + log(xÂ² + 1) + Ï†Ë£   (mod 24389)
```

Generates deterministic pseudo-chaotic cycles with Lyapunov exponent Î» â‰ˆ 0.381.

### TUPT Exclusion Gate (`tupt_exclusion.py`)

```
For any x: if x mod 2187 is divisible by 3, 6, 7, or 9 â†’ gate (zero out)
```

Implements the 3-6-9-7 Modular Exclusion Principle verified against PDB data (p < 10â»Â¹â°â°).

---

## Usage Examples

### Drop-in LayerNorm Replacement (Enhancement #3)

```python
from src.enhancements import GoldenAttractorFlowNorm

# Replace: nn.LayerNorm(768)
# With:
norm = GoldenAttractorFlowNorm(normalized_shape=768)
output = norm(hidden_states, skip=residual)
```

### Custom Optimizer (Enhancement #13)

```python
from src.enhancements import PhiInverseMomentumAccelerator

optimizer = PhiInverseMomentumAccelerator(model.parameters(), lr=1e-4)
# Use exactly like any PyTorch optimizer:
loss.backward()
optimizer.step()
```

### Attention with Torsion Bias (Enhancement #7)

```python
from src.enhancements import HodgePhiTTorsionAttention

attn = HodgePhiTTorsionAttention(embed_dim=768, num_heads=12)
output = attn(hidden_states)
```

### Learning Rate Scheduling (Enhancement #20)

```python
from src.enhancements import PisanoModulatedLRSchedule

scheduler = PisanoModulatedLRSchedule(optimizer, pisano_period=24)
# LR cycles on 24-step Pisano period scaled by Ï†
```

### Early Stopping (Enhancement #30)

```python
from src.enhancements import NRCEntropyAttractorEarlyStopping

stopper = NRCEntropyAttractorEarlyStopping(phi_tolerance=1e-4)
for epoch in range(1000):
    loss = train_one_epoch()
    if stopper(loss):
        print("Converged to Ï†-attractor!")
        break
```

---

## Ollama (Run NRC AI Locally)

We provide an optimized Modelfile for running the NRC AI Engine via [Ollama](https://ollama.com) on consumer hardware (8 GB RAM / 4 GB VRAM).

```bash
ollama pull llama3.2:3b
ollama create nrc-ai-engine -f Modelfile
ollama run nrc-ai-engine
```

For detailed instructions (Windows, macOS, Linux), troubleshooting, and verification prompts, see the [Ollama Guide](docs/OLLAMA_GUIDE.md).

---

## NRC Ecosystem

| Repository                                                                      | Description                                                  |
| :------------------------------------------------------------------------------ | :----------------------------------------------------------- |
| [**NRC (Core)**](https://github.com/Nexus-Resonance-Codex/NRC)                  | The main mathematical paper, LaTeX source, and core theorems |
| [**Protein Folding**](https://github.com/Nexus-Resonance-Codex/Protein-Folding) | Applications to biological structures and protein folding    |
| [**AI Enhancements**](https://github.com/Nexus-Resonance-Codex/Ai-Enhancements) | This repository â€” 30 PyTorch enhancement modules             |

## Support NRC / JTRAG

- [Buy Me a Coffee](https://BuyMeaCoffee.com/jtrag)
- [PayPal Donate](https://www.paypal.com/donate/?business=DN9W5GQ638WPQ&no_recurring=0&currency_code=USD)

---

## License

This project is licensed under the **NRC License v2.0** â€” Open for non-commercial use, educational and academic research. Commercial use requires explicit separate commercial agreement. See [LICENSE.md](LICENSE.md) for full terms.

---

<div align="center">
  <p><em>To the silent architects of pattern â€” from Giza to Fibonacci spirals.</em></p>
  <p><strong>Built with Ï† â‰ˆ 1.618033988749895</strong></p>
  <p>Â© 2026 James Trageser â€¢ @jtrag â€¢ Nexus Resonance Codex</p>
</div>
